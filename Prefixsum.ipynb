{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7ZW8UiiIxi6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e015395e-de7b-4123-9590-4ebc981a4af3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting prefix.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile prefix.cu\n",
        "#include <iostream>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// Kernel for parallel prefix sum\n",
        "__global__ void prefixSum(int *input, int *output, int n) {\n",
        "    extern __shared__ int temp[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    // Load input into shared memory\n",
        "    temp[tid] = (idx < n) ? input[idx] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Reduction phase\n",
        "    for (int stride = 1; stride < blockDim.x; stride *= 2) {\n",
        "        int index = 2 * stride * (tid + 1) - 1;\n",
        "        if (index < blockDim.x) {\n",
        "            temp[index] += temp[index - stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Downsweep phase\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {\n",
        "        int index = 2 * stride * (tid + 1) - 1;\n",
        "        if (index + stride < blockDim.x) {\n",
        "            temp[index + stride] += temp[index];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Store result to output\n",
        "    if (idx < n) {\n",
        "        output[idx] = temp[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Wrapper function for inclusive prefix sum calculation\n",
        "void inclusivePrefixSum(int *d_input, int *d_output, int n) {\n",
        "    int numBlocks = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    // Create CUDA events for timing\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Record start time\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    // Perform parallel prefix sum\n",
        "    prefixSum<<<numBlocks, BLOCK_SIZE, BLOCK_SIZE * sizeof(int)>>>(d_input, d_output, n);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Record stop time\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // Calculate elapsed time\n",
        "    float milliseconds = 0;\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\n",
        "   std::cout << \"Execution Time: \" << milliseconds * 1000<< \" microseconds\" << std::endl;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 5; // Size of the array (assuming 32n elements)\n",
        "    int arraySize = 32 * n;\n",
        "\n",
        "    // Host array\n",
        "    int *h_input = new int[arraySize];\n",
        "\n",
        "    // Seed for random number generation\n",
        "    srand(time(nullptr));\n",
        "\n",
        "    // Generate random numbers for the array\n",
        "    for (int i = 0; i < arraySize; ++i) {\n",
        "        h_input[i] = rand() % 100; //\n",
        "    }\n",
        "\n",
        "    // Device arrays\n",
        "    int *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, arraySize * sizeof(int));\n",
        "    cudaMalloc(&d_output, arraySize * sizeof(int));\n",
        "\n",
        "    // Copy data from host to device\n",
        "    cudaMemcpy(d_input, h_input, arraySize * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Perform inclusive prefix sum\n",
        "    inclusivePrefixSum(d_input, d_output, arraySize);\n",
        "\n",
        "    // Copy result back to host\n",
        "    int *h_output = new int[arraySize];\n",
        "    cudaMemcpy(h_output, d_output, arraySize * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print results\n",
        "    std::cout << \"Input array: \";\n",
        "    for (int i = 0; i < arraySize; ++i) {\n",
        "        std::cout << h_input[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    std::cout << \"Inclusive Prefix Sum: \";\n",
        "    for (int i = 0; i < arraySize; ++i) {\n",
        "        std::cout << h_output[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    // Clean up\n",
        "    delete[] h_input;\n",
        "    delete[] h_output;\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o prefix prefix.cu\n",
        "!./prefix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQIIR87bdVcB",
        "outputId": "1d733f12-e3d0-4bc6-a909-5fe31fd83255"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 178.176 microseconds\n",
            "Input array: 55 68 31 25 16 96 33 71 91 41 68 85 63 47 14 89 10 83 56 61 82 46 38 51 1 47 31 90 9 49 38 16 17 21 41 33 69 26 4 12 67 24 49 30 23 15 19 33 99 28 95 81 74 85 85 27 84 68 17 94 17 7 10 86 80 52 71 49 78 27 61 98 3 62 28 26 78 0 12 29 28 59 62 54 44 47 81 29 15 51 75 84 58 85 22 39 89 93 40 20 72 54 18 75 16 98 54 94 50 66 75 30 25 38 84 21 37 18 2 5 69 77 41 27 15 64 18 4 9 59 24 82 13 42 9 81 93 63 28 43 81 3 74 58 93 10 32 31 28 34 88 97 64 29 77 79 45 95 83 55 \n",
            "Inclusive Prefix Sum: 55 123 154 179 195 291 324 395 486 527 595 680 743 790 804 893 903 986 1042 1103 1185 1231 1269 1320 1321 1368 1399 1489 1498 1547 1585 1601 1618 1639 1680 1713 1782 1808 1812 1824 1891 1915 1964 1994 2017 2032 2051 2084 2183 2211 2306 2387 2461 2546 2631 2658 2742 2810 2827 2921 2938 2945 2955 3041 3121 3173 3244 3293 3371 3398 3459 3557 3560 3622 3650 3676 3754 3754 3766 3795 3823 3882 3944 3998 4042 4089 4170 4199 4214 4265 4340 4424 4482 4567 4589 4628 4717 4810 4850 4870 4942 4996 5014 5089 5105 5203 5257 5351 5401 5467 5542 5572 5597 5635 5719 5740 5777 5795 5797 5802 5871 5948 5989 6016 6031 6095 6113 6117 6126 6185 6209 6291 6304 6346 6355 6436 6529 6592 6620 6663 6744 6747 6821 6879 6972 6982 7014 7045 7073 7107 7195 7292 7356 7385 7462 7541 7586 7681 7764 7819 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt update\n",
        "!apt install ./nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt --fix-broken install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap-uagNPilJa",
        "outputId": "f34daf2b-4c95-4a4e-ce38-1fbb0b0afd03"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-21 08:54:55--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.20.126\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.20.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 317705436 (303M) [application/x-deb]\n",
            "Saving to: ‘nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb.1’\n",
            "\n",
            "nsight-systems-2023 100%[===================>] 302.99M   274MB/s    in 1.1s    \n",
            "\n",
            "2023-12-21 08:54:56 (274 MB/s) - ‘nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb.1’ saved [317705436/317705436]\n",
            "\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "28 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'nsight-systems-2023.2.3' instead of './nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb'\n",
            "nsight-systems-2023.2.3 is already the newest version (2023.2.3.1001-32894139v0).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_code =\"\"\" #include <iostream>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// Kernel for parallel prefix sum\n",
        "__global__ void prefixSum(int *input, int *output, int n) {\n",
        "    extern __shared__ int temp[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    // Load input into shared memory\n",
        "    temp[tid] = (idx < n) ? input[idx] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Reduction phase\n",
        "    for (int stride = 1; stride < blockDim.x; stride *= 2) {\n",
        "        int index = 2 * stride * (tid + 1) - 1;\n",
        "        if (index < blockDim.x) {\n",
        "            temp[index] += temp[index - stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Downsweep phase\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {\n",
        "        int index = 2 * stride * (tid + 1) - 1;\n",
        "        if (index + stride < blockDim.x) {\n",
        "            temp[index + stride] += temp[index];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Store result to output\n",
        "    if (idx < n) {\n",
        "        output[idx] = temp[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Wrapper function for inclusive prefix sum calculation\n",
        "void inclusivePrefixSum(int *d_input, int *d_output, int n) {\n",
        "    int numBlocks = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    // Create CUDA events for timing\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Record start time\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    // Perform parallel prefix sum\n",
        "    prefixSum<<<numBlocks, BLOCK_SIZE, BLOCK_SIZE * sizeof(int)>>>(d_input, d_output, n);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Record stop time\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // Calculate elapsed time\n",
        "    float milliseconds = 0;\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\n",
        "   std::cout << \"Execution Time: \" << milliseconds * 1000<< \" microseconds\" << std::endl;\n",
        "}\n",
        "int main() {\n",
        "    int n = 5;\n",
        "    int arraySize = 32 * n;\n",
        "\n",
        "    // Host array\n",
        "    int *h_input = new int[arraySize];\n",
        "\n",
        "    // Seed for random number generation\n",
        "    srand(time(nullptr));\n",
        "\n",
        "    // Generate random numbers for the array\n",
        "    for (int i = 0; i < arraySize; ++i) {\n",
        "        h_input[i] = rand() % 100; //\n",
        "    }\n",
        "\n",
        "    // Device arrays\n",
        "    int *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, arraySize * sizeof(int));\n",
        "    cudaMalloc(&d_output, arraySize * sizeof(int));\n",
        "\n",
        "    // Copy data from host to device\n",
        "    cudaMemcpy(d_input, h_input, arraySize * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Perform inclusive prefix sum\n",
        "    inclusivePrefixSum(d_input, d_output, arraySize);\n",
        "\n",
        "    // Copy result back to host\n",
        "    int *h_output = new int[arraySize];\n",
        "    cudaMemcpy(h_output, d_output, arraySize * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print results\n",
        "    std::cout << \"Input array: \";\n",
        "    for (int i = 0; i < arraySize; ++i) {\n",
        "        std::cout << h_input[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    std::cout << \"Inclusive Prefix Sum: \";\n",
        "    for (int i = 0; i < arraySize; ++i) {\n",
        "        std::cout << h_output[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    // Clean up\n",
        "    delete[] h_input;\n",
        "    delete[] h_output;\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "with open('prefix.cu', 'w') as f:\n",
        "    f.write(cuda_code)\n",
        "\n",
        "# Compile the CUDA code\n",
        "!nvcc -o prefix prefix.cu\n",
        "\n",
        "# Run the executable with Nsight Systems profiling\n",
        "!nsys profile -t cuda --stats=true ./prefix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98k5kE_0egni",
        "outputId": "2b267944-a2f5-44e0-9486-c95f355f69cd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 248.736 microseconds\n",
            "Input array: 68 66 58 12 2 96 42 39 34 34 62 47 85 90 98 66 13 30 14 6 77 8 80 49 99 90 99 79 85 86 82 53 4 40 18 58 89 60 97 23 46 11 23 84 1 73 2 67 3 16 25 32 24 57 33 23 47 32 54 84 70 37 90 74 29 8 32 18 20 82 94 18 45 17 2 99 42 4 66 45 72 91 29 48 48 62 24 47 46 78 32 68 67 74 42 49 82 26 19 54 60 13 72 6 82 27 5 24 83 23 21 8 14 2 56 14 64 80 13 62 11 45 30 78 19 24 79 53 3 99 59 63 12 84 21 47 63 78 23 46 1 45 54 67 47 11 81 64 43 47 26 54 92 9 85 64 33 64 69 88 \n",
            "Inclusive Prefix Sum: 68 134 192 204 206 302 344 383 417 451 513 560 645 735 833 899 912 942 956 962 1039 1047 1127 1176 1275 1365 1464 1543 1628 1714 1796 1849 1853 1893 1911 1969 2058 2118 2215 2238 2284 2295 2318 2402 2403 2476 2478 2545 2548 2564 2589 2621 2645 2702 2735 2758 2805 2837 2891 2975 3045 3082 3172 3246 3275 3283 3315 3333 3353 3435 3529 3547 3592 3609 3611 3710 3752 3756 3822 3867 3939 4030 4059 4107 4155 4217 4241 4288 4334 4412 4444 4512 4579 4653 4695 4744 4826 4852 4871 4925 4985 4998 5070 5076 5158 5185 5190 5214 5297 5320 5341 5349 5363 5365 5421 5435 5499 5579 5592 5654 5665 5710 5740 5818 5837 5861 5940 5993 5996 6095 6154 6217 6229 6313 6334 6381 6444 6522 6545 6591 6592 6637 6691 6758 6805 6816 6897 6961 7004 7051 7077 7131 7223 7232 7317 7381 7414 7478 7547 7635 \n",
            "Generating '/tmp/nsys-report-ac6d.qdstrm'\n",
            "[1/6] [========================100%] report1.nsys-rep\n",
            "[2/6] [========================100%] report1.sqlite\n",
            "[3/6] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)    Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  ------------  --------  -----------  ------------  ----------------------\n",
            "     99.6      117,193,285          2  58,596,642.5  58,596,642.5     6,769  117,186,516  82,858,593.7  cudaMalloc            \n",
            "      0.2          230,544          1     230,544.0     230,544.0   230,544      230,544           0.0  cudaLaunchKernel      \n",
            "      0.1          134,524          2      67,262.0      67,262.0    11,627      122,897      78,679.8  cudaFree              \n",
            "      0.1           65,808          2      32,904.0      32,904.0    28,289       37,519       6,526.6  cudaMemcpy            \n",
            "      0.0           19,416          2       9,708.0       9,708.0     1,241       18,175      11,974.1  cudaEventCreate       \n",
            "      0.0           16,079          2       8,039.5       8,039.5     3,813       12,266       5,977.2  cudaEventRecord       \n",
            "      0.0            9,929          1       9,929.0       9,929.0     9,929        9,929           0.0  cudaDeviceSynchronize \n",
            "      0.0            3,688          1       3,688.0       3,688.0     3,688        3,688           0.0  cudaEventSynchronize  \n",
            "      0.0            1,587          1       1,587.0       1,587.0     1,587        1,587           0.0  cuModuleGetLoadingMode\n",
            "\n",
            "[4/6] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Name            \n",
            " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------------\n",
            "    100.0            7,296          1   7,296.0   7,296.0     7,296     7,296          0.0  prefixSum(int *, int *, int)\n",
            "\n",
            "[5/6] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)      Operation     \n",
            " --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------\n",
            "     64.8            2,176      1   2,176.0   2,176.0     2,176     2,176          0.0  [CUDA memcpy DtoH]\n",
            "     35.2            1,184      1   1,184.0   1,184.0     1,184     1,184          0.0  [CUDA memcpy HtoD]\n",
            "\n",
            "[6/6] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
            "      0.001      1     0.001     0.001     0.001     0.001        0.000  [CUDA memcpy DtoH]\n",
            "      0.001      1     0.001     0.001     0.001     0.001        0.000  [CUDA memcpy HtoD]\n",
            "\n",
            "Generated:\n",
            "    /content/report1.nsys-rep\n",
            "    /content/report1.sqlite\n"
          ]
        }
      ]
    }
  ]
}